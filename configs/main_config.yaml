# @package _global_

defaults:
  - data: default
  - model: cnnlstm
  - training: default
  - trainer: default
  # The sweeper will be explicitly provided via command line with -m
  - _self_

# Global configs
seed: 42

# Wandb logging, all arguments are only used if use_wandb is set to true
use_wandb: true
wandb_project: "cse-151b-competition"
wandb_entity: "saem-uc-san-diego"
run_name: "optuna_CNNLSTM_s${data.sequence_length}_b${data.batch_size}_L${model.lstm_hidden_dim}x${model.n_lstm_layers}dp${model.lstm_dropout}_C${model.cnn_init_dim}x${model.cnn_depth}k${model.cnn_kernel_size}dp${model.cnn_dropout_rate}_lr${training.lr}_wd${training.weight_decay}_pat${trainer.callbacks.2.patience}"
wandb_group: "Hyperparameter_Sweep"

# Optuna metric to optimize
optimized_metric: val/loss

# Path to a checkpoint to load. If set, will load this checkpoint and resume training (evaluation)
ckpt_path: null

hydra:
  job:
    chdir: true
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}_${hydra.job.name}
    subdir: ${hydra.job.num}
